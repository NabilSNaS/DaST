# DaST: Data-free Substitute Training for Adversarial Attacks
This repository contains an implementation of DaST: Data-Free Substitute Training for Adversarial Attacks, originally developed by Zhou et al., CVPR 2020. The method enables adversarial black-box attacks without using any real data, making it ideal for security and privacy analysis of deployed machine learning models.

Source GitHub Repository: https://github.com/zhoumingyi/DaST

***Abstract***: Machine learning models are vulnerable to adversarial examples. For the black-box setting, current substitute attacks need pre-trained models to generate adversarial examples. However, pre-trained models are hard to obtain in real-world tasks. In this paper, we propose a data-free substitute training method (DaST) to obtain substitute models for adversarial black-box attacks without the requirement of any real data. To achieve this, DaST utilizes specially designed generative adversarial networks (GANs) to train the substitute models. In particular, we design a multi-branch architecture and label-control loss for the generative model to deal with the uneven distribution of synthetic samples. The substitute model is then trained by the synthetic samples generated by the generative model, which are labeled by the attacked model subsequently. The experiments demonstrate the substitute models produced by DaST can achieve competitive performance compared with the baseline models which are trained by the same train set with attacked models. Additionally, to evaluate the practicability of the proposed method on the real-world task, we attack an online machine learning model on the Microsoft Azure platform. The remote model misclassifies 98.35% of the adversarial examples crafted by our method. To the best of our knowledge, we are the first to train a substitute model for adversarial attacks without any real data.

*Read the full paper here: https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_DaST_Data-Free_Substitute_Training_for_Adversarial_Attacks_CVPR_2020_paper.html*


# Environment & Installation

The performance of DaST has been found to vary across different machines, even when all random seeds are set. All required libraries are listed in the requirements.txt file. A virtual environment can be created to ensure reproducibility:

```bash
python -m venv env
source env/bin/activate  # or `env\Scripts\activate` on Windows
pip install -r requirements.txt
```
# Experimental Setup

- `Python 3.12.9`
- `PyTorch 2.6.0` with `CUDA 12.4`
- `torchvision==0.21.0`
- `foolbox==3.3.4`
- `advertorch==0.2.3`
- `numpy==2.2.4`
- `scikit-learn==1.6.1`
- `scipy==1.15.2`
- `joblib==1.4.2`
- `pillow==11.2.1`
  
 **Note**: The original implementation of DaST, as described in the [CVPR 2020 paper](https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_DaST_Data-Free_Substitute_Training_for_Adversarial_Attacks_CVPR_2020_paper.html), was built using **PyTorch 1.0+**. This version has been adapted to work with **PyTorch 2.6.0**.

Additional packages such as `eagerpy`, `docopt`, `matplotlib-inline`, and `Jinja2` are also included and can be installed via the `requirements.txt` file.

# Hardware

- `GPU: NVIDIA GeForce RTX 2080 Ti (11GB, Turing architecture)`
- `CUDA Version: 12.4`
- `Driver Version: 550.127.08`


This work can steal the attacked model without the requirement of any real data. If you want to evaluate the performance of DaST in terms of adversarial attacks, you can use the `evaluate.py` to do it.

# How it works
In the DaST framework, a substitute model is trained to imitate a black-box target model without using real data. Here's the basic idea:
1. A GAN-based generator produces synthetic samples.
2. These samples are fed into the target model to obtain the labels.
3. A substitute model is trained on these synthetic samples and labels.
4. Adversarial attacks are generated on the substitute and transferred to the target.

# Architecture
   
**MNIST Dataset:**
- Target model: medium-sized CNN

- Substitute model: larger CNN
  
**CIFAR-10 Dataset:**
- Target model: VGG-16

- Substitute models: VGG-13, ResNet-18, ResNet-50

The substitute model mimics the decision boundaries of the target model using only synthetic data and black-box queries.


# Experiments

1. Train the Target Model (VGG16 on CIFAR-10)
```python
python train_vgg_cifar10.py
```
This generates vgg_vgg16_best.pth, which simulates the black-box target model for DaST training. 
**Note:**  This script was newly developed as it was not included in the original repository.

Now, train the substitute model using DaST by querying the target model without real data:
```python
python dast_cifar10.py
```

If you want to train a substitute model in MNIST:

```python
python dast.py --dataset=mnist
```

If you want to train a subsitute model in Azure:
```python
python dast.py --dataset=azure
```
2. Train to get the the Pretrained ResNet-50 Model on CIFAR-10:
```python
python resnet50_cifar10_train.py
```
This will generate a path as resnet_50_cifar10.pth in the pretrained folder. This model will be used for baseline adversarial attack evaluations.

3. Once the substitute model is obtained, generate adversarial examples and evaluate their performance in non-targeted attacks
```python
python eval_resnet.py --mode=dast --adv=FGSM --cuda
```
Evaluate Pretrained ResNet-50 as the baseline model:
FGSM Attack:
```python
python eval_resnet.py  --mode=baseline --adv=FGSM --baseline-model=pretrained/resnet50_cifar10.pth
```
PGD Attack:
```python
python eval_resnet.py  --mode=baseline --adv=PGD --baseline-model=pretrained/resnet50_cifar10.pth
```
BIM Attack:
```python
python eval_resnet.py  --mode=baseline --adv=BIM --baseline-model=pretrained/resnet50_cifar10.pth
```
4. Explore Improved Generative Models (DCGAN Version)
Train DaST using DCGAN for improved diversity:
```
python dast_cifar10_dcgan.py --batchSize=128
```
After training:

Update model paths (DaST Model, Baseline Model, and Target Model) in eval_resnet.py.Trained models will be saved in the saved_model and pretrained directories.

Then evaluate the DCGAN-trained model:
```
python eval_resnet.py  --mode=baseline --adv=PGD --baseline-model=pretrained/resnet50_cifar10.pth
```

# Notes

(1) A downloaded copy of the Azure model is included; no need to deploy the model separately.

(2) The attack success rate shown during training is an estimate. For proper evaluation, run evaluation.py to evaluate the performance of trained model.

(3) If adapting DaST to other datasets (like CIFAR-10), add a CIFAR-10's model as the original_net and load the dataset. Note that the generator output must be resized from [28x28] to [32x32].

(4) Key hyperparameters:

- alpha: Controls the weight of label-control loss in (9) of the original paper.

- beta: Determines the attack scenario. If the 'beta' is 0, the attack scenario is DaST-L, if the 'beta' is not 0 (>0), the attack scenario is DaST-P.

- G_type: There are two types of generator architecture, switch by 'G_type
**Note:** It is hard to say which type is better, can try to use them in your own dataset. Because of the multi-branch architecture of the generator, the 'batchsize' is best divisible by the number of categories.

(5) Training instability has been observed on CIFAR-10 despite fixed seeds. Check logs for details. Performance may vary (The first example is failed, the training is collapsed. The second example is better, the attack success rate increases to 80% after 50 epochs in DaST-P. )


# Citation:
If you find this work useful, please cite the original authors:
```latex
@inproceedings{zhou2020dast,
  title={DaST: Data-free Substitute Training for Adversarial Attacks},
  author={Zhou, Mingyi and Wu, Jing and Liu, Yipeng and Liu, Shuaicheng and Zhu, Ce},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={234--243},
  year={2020}
}
```

# Contact:
For questions related to this implementation: Hafija Aktar — hafijaaktar@uri.edu

For original paper inquiries: Mingyi Zhou — mingyi.zhou@monash.edu
